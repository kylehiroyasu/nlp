{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we define words in NLP and how does that effect our machine learning model and the downstream task.\n",
    "\n",
    "## Words\n",
    "\n",
    "Deciding what counts as a word can depend heavily on the downstream task. For topic modeling or text classification punctuation may not play a big role, however things like sentiment analysis, speech to text, or parsing linguistic features may rely on such information. Furthermore, in speech to text, one might encounter words like *uh* or *um* which may or may not be useful.\n",
    "\n",
    "Further complications can be things like the casing of a word, *they* versus *They* or the inflection of words like *cats* verus *cat*.  A **lemma** refers to sets of inflected word forms which share the same stem, part of speech and word sense. In contrast **word form** is the inflected form, the natural variations of words that come up in language.\n",
    "\n",
    "The english language is rather forgiving in this sense, because a given **lemma** tends to have a limited number of **word forms**. In contrast languages like **Arabic** or **Turkish** rely on a large variety of **word forms** for any given **lemma**. This can be more challenging because if these tokens are treated indepently, a model has to recover the relationship between a wide array possible **word forms**.\n",
    "\n",
    "One phenomena that comes up in english corpora is called **Herdan's Law** or **Heap's Law** which implies that the  size of the vocabulary for a text goes up significantly fast than the square root of its length in words. Namely for `V` the vocabulary (i.e. unique words) of a corpora, where the corpora consists of `N` tokens (the individual, non-unique words of a text) the following holds:\n",
    "\n",
    "```\n",
    "|V| = kN^b\n",
    "```\n",
    " where `k` and `b` are positive constants and `0 < b < 1` (usually ~ 0.65-0.75). This pattern will come up when we experiment with tokenization in the next section.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Strictly speaking, things like tokenization can be done via hand written regexp and bash scripts, however there now exist libraries like spaCy which make the process of tokenization and extraction of linguistic features much easier. In addition, to overcome limitations of more complex languages and their morphological features, sub-word tokenization approaches have also been developed.\n",
    "\n",
    "Nonetheless this isn't a solved problem. Because this is one of the first steps in preparing text for NLP applications, decisions here have long lasting decisions in the downstream model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "As part of a standard text normalization process the following likely arise:\n",
    "\n",
    "- Tokenization\n",
    "- Normalizing word formats\n",
    "- Segmenting sentences\n",
    "\n",
    "If you read enough NLP papers, especially more classical approaches, you will observe that significant thought and effort can be put into optimizing your normalization approach for downstream tasks. This might include any or more of the following:\n",
    "\n",
    "- making tokenization exceptions for unusual patterns specific to your corpus which might otherwise indicate word boundaries (i.e. 'New York' or 'New' 'York')\n",
    "- accepting punctuation or not as a token\n",
    "- replacing URLs or money sums with special tokens like #URL and #MONEY\n",
    "- splitting tokens like \"can't\" to \"can\" \"'t\"\n",
    "- lowercasing all text\n",
    "\n",
    "Each of these decisions may have practical or performance related impacts which one should think about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "Here is a small example comparing the tokenization of a couple different approaches to see how a document might get tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install a spacy model if necessary:\n",
    "\n",
    "```\n",
    "!python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kylehiroyasu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Speed Performance\n",
    "\n",
    "Here we compare the speed of the `nltk`  tokenizers with a simple `regex` and with the built in `spaCy` tokenizer as well. First we'll see how quickly this process takes on a sample news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 0 ns, total: 22.4 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_tokenizer_count = Counter()\n",
    "\n",
    "for text in texts:\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    word_tokenizer_count.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 s, sys: 666 Âµs, total: 1.5 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wordpunct_tokenizer_count = Counter()\n",
    "\n",
    "for text in texts:\n",
    "    tokens = nltk.tokenize.wordpunct_tokenize(text)\n",
    "    wordpunct_tokenizer_count.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 0 ns, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nltk_regex_tokenizer_count = Counter()\n",
    "\n",
    "for text in texts:\n",
    "    tokens = nltk.tokenize.regexp_tokenize(text,\"[\\w']+\")\n",
    "    nltk_regex_tokenizer_count.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 54.8 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spacy_tokenizer_count = Counter()\n",
    "\n",
    "for text in tokenizer.pipe(texts):\n",
    "    spacy_tokenizer_count.update([t.text for t in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>unique tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word</td>\n",
       "      <td>204783</td>\n",
       "      <td>4790508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wordpunct</td>\n",
       "      <td>173119</td>\n",
       "      <td>5051134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regex</td>\n",
       "      <td>163358</td>\n",
       "      <td>3663948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spaCy</td>\n",
       "      <td>282056</td>\n",
       "      <td>3811544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  unique tokens  total_tokens\n",
       "0       word         204783       4790508\n",
       "1  wordpunct         173119       5051134\n",
       "2      regex         163358       3663948\n",
       "3      spaCy         282056       3811544"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters = [word_tokenizer_count, wordpunct_tokenizer_count, nltk_regex_tokenizer_count, spacy_tokenizer_count]\n",
    "names = ['word', 'wordpunct', 'regex','spaCy']\n",
    "results = []\n",
    "for name, count in zip(names, counters):\n",
    "    results.append({\n",
    "        'name':name,\n",
    "        'unique tokens': len(count.keys()),\n",
    "        'total_tokens': sum(count.values())\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'word'\n",
      "[('>', 187306),\n",
      " (',', 161254),\n",
      " ('.', 143573),\n",
      " ('the', 129570),\n",
      " ('--', 116980),\n",
      " (':', 113896),\n",
      " (')', 71944),\n",
      " ('to', 71194),\n",
      " ('(', 70544),\n",
      " ('*', 68230)]\n",
      "'wordpunct'\n",
      "[('.', 282553),\n",
      " (',', 151604),\n",
      " ('the', 129745),\n",
      " (':', 108345),\n",
      " ('>', 71695),\n",
      " ('to', 71604),\n",
      " ('-', 70394),\n",
      " ('of', 67699),\n",
      " (\">'\", 66852),\n",
      " ('AX', 62396)]\n",
      "'regex'\n",
      "[('the', 129708),\n",
      " ('to', 71596),\n",
      " ('of', 67699),\n",
      " (\"'AX\", 61917),\n",
      " ('a', 57392),\n",
      " ('and', 54634),\n",
      " ('I', 43987),\n",
      " ('is', 41377),\n",
      " ('in', 39257),\n",
      " ('that', 36498)]\n",
      "'spaCy'\n",
      "[('\\n', 289433),\n",
      " ('the', 127670),\n",
      " (' ', 78811),\n",
      " ('to', 69836),\n",
      " ('of', 66705),\n",
      " ('a', 56148),\n",
      " ('\\n\\n', 55935),\n",
      " ('and', 52580),\n",
      " ('is', 39554),\n",
      " ('in', 37787)]\n"
     ]
    }
   ],
   "source": [
    "for name, count in zip(names, counters):\n",
    "    pprint(name)\n",
    "    pprint(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even for a relatively small corpus the tokenization method you choose can have a large impact on the resulting vocabulary and token counts. Furthermore, depending on the scale of the data, some approaches will take substantially longer than others. It should be noted that although `spaCy` appears to be much slower, it provides a much greater range of linguistic features out of the box, thus if you need more linguistic nature this library will make the most sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencization\n",
    "\n",
    "Another often important aspect of preprocessing texts is breaking sentences into their respective sentences. While usually looking for a period is a good start, there are a great deal of exceptions to this rule which make this process rather complicated. To show the variation in results here we can again compare the `NLTK` and `spaCy` results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.07 s, sys: 35.8 ms, total: 5.11 s\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nltk_sentences = set()\n",
    "nltk_count = 0\n",
    "for text in texts:\n",
    "    #sents = nltk.tokenize.sent_tokenize(text)\n",
    "    sents = [s.strip() for s in nltk.tokenize.sent_tokenize(text)]\n",
    "    nltk_count += len(sents)\n",
    "    nltk_sentences.update(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 s, sys: 31.3 ms, total: 31.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_sentences = set()\n",
    "spacy_count = 0\n",
    "for text in texts:\n",
    "    #sents = [s.text for s in list(nlp(text).sents)]\n",
    "    sents = [s.text.strip() for s in list(nlp(text).sents)]\n",
    "    spacy_count += len(sents)\n",
    "    spacy_sentences.update(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of sentences NLTK : 187884 spaCy: 189612'\n"
     ]
    }
   ],
   "source": [
    "pprint(u\"Number of sentences NLTK : {} spaCy: {}\".format(nltk_count, spacy_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of unique sentences NLTK : 167792 spaCy: 165475'\n"
     ]
    }
   ],
   "source": [
    "pprint(u\"Number of unique sentences NLTK : {} spaCy: {}\".format(len(nltk_sentences), len(spacy_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of identical sentences: 146573'\n"
     ]
    }
   ],
   "source": [
    "intersection = len(nltk_sentences.intersection(spacy_sentences))\n",
    "pprint(\"Number of identical sentences: {}\".format(intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see here that there appears to be a discrepancy in the number of sentences found by the two libraries, and an even greater discrepancy in how the sentence must be getting parsed. If this step is crucial to your application, it is certainly worthwhile to understand the differences, and if either approach is appropriate for your corpus. If the text you are trying process has nonstandardized text you may need to find your own solution, fix texts before **sentencizing** them, or write `spaCy` exceptions to correct these patterns.\n",
    "\n",
    "Here is an example text to compare the results of the two **sentencizers**. In this case we see that white space is being handled differently between the two libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: gunning@cco.caltech.edu (Kevin J. Gunning)\\n'\n",
      " 'Subject: stolen CBR900RR\\n'\n",
      " 'Organization: California Institute of Technology, Pasadena\\n'\n",
      " 'Lines: 12\\n'\n",
      " 'Distribution: usa\\n'\n",
      " 'NNTP-Posting-Host: alumni.caltech.edu\\n'\n",
      " 'Summary: see above\\n'\n",
      " '\\n'\n",
      " 'Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.',\n",
      " 'Blue and white Honda CBR900RR california plate KG CBR.',\n",
      " 'Serial number\\nJH2SC281XPM100187, engine number 2101240.',\n",
      " 'No turn signals or mirrors, lights taped over for track riders session\\n'\n",
      " 'at Willow Springs tomorrow.',\n",
      " \"Guess I'll miss it.\",\n",
      " ':-(((\\n\\nHelp me find my baby!!!',\n",
      " 'kjg']\n"
     ]
    }
   ],
   "source": [
    "pprint([s.strip() for s in nltk.tokenize.sent_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: gunning@cco.caltech.edu (Kevin J. Gunning)\\n'\n",
      " 'Subject: stolen CBR900RR\\n'\n",
      " 'Organization: California Institute of Technology, Pasadena\\n'\n",
      " 'Lines: 12\\n'\n",
      " 'Distribution: usa\\n'\n",
      " 'NNTP-Posting-Host: alumni.caltech.edu\\n'\n",
      " 'Summary: see above\\n'\n",
      " '\\n'\n",
      " 'Stolen from Pasadena between 4:30 and 6:30 pm on 4/15.',\n",
      " 'Blue and white Honda CBR900RR california plate KG CBR.',\n",
      " 'Serial number\\nJH2SC281XPM100187, engine number 2101240.',\n",
      " 'No turn signals or mirrors, lights taped over for track riders session\\n'\n",
      " 'at Willow Springs tomorrow.',\n",
      " \"Guess I'll miss it.\",\n",
      " ':-(((\\n\\nHelp me find my baby!!!',\n",
      " 'kjg']\n"
     ]
    }
   ],
   "source": [
    "pprint([s.text.strip() for s in list(nlp(text).sents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: gunning@cco.caltech.edu (Kevin J. Gunning)\\nSubject: stolen CBR900RR\\nOrganization: California Institute of Technology, Pasadena\\nLines: 12\\nDistribution: usa\\nNNTP-Posting-Host: alumni.caltech.edu\\nSummary: see above\\n\\nStolen from Pasadena between 4:30 and 6:30 pm on 4/15.',\n",
       " 'Blue and white Honda CBR900RR california plate KG CBR.',\n",
       " 'Serial number\\nJH2SC281XPM100187, engine number 2101240.',\n",
       " 'No turn signals or mirrors, lights taped over for track riders session\\nat Willow Springs tomorrow.',\n",
       " \"Guess I'll miss it.\",\n",
       " ':-(((\\n\\nHelp me find my baby!!!',\n",
       " 'kjg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s.text.strip() for s in list(nlp(text).sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair and Wordpiece Encoding\n",
    "\n",
    "Two additional forms of tokenization which relax the strict focus on word boundaries are byte-pair encoding and wordpiece encoding. The advantage of such systems is that their relaxation of word boundaries means that we might be able to find common subwords, or even using word parts, understand new words which weren't present in the training corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-Pair Encoding\n",
    "\n",
    "Byte-Pair encoding is deemed as one of the simplest methods of finding this new word encoding. In order to build a vocabulary by which the tokenizer can split a corpus, the algorithm starts by creating a list of all character pairs in the corpus. Then the most common pair and merged to create a new \"character\" and the process is repeated. This approach however assumes that individual words have already been tokenized into their indvidial words, this prevents us from merging characters across words. Finally, the merging process terminates after some arbitrary numer of merges which can be seen as a hyperparameter in the training process.\n",
    "\n",
    "Below we will try to implement an example which illustrates the process. First we'll tokenize our data as above, however not sentencize it, then we can begin the byte-pair encoding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_corpus  = []\n",
    "\n",
    "for text in tokenizer.pipe(texts):\n",
    "    bpe_corpus.append([t.text for t in text if not t.is_punct and not t.is_space])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:', 'irwin@cmptrc.lonestar.org', '(Irwin', 'Arnstein)', 'Subject:']\n"
     ]
    }
   ],
   "source": [
    "pprint(bpe_corpus[10][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at this point we have a list of lists which contain our text. Now we we want to start counting the frequency of character pairs. One exception, is that what happens to single character words like \"I\" or \"a\". Actually, in BPE we append a special character to the end of each word as a place holder.\n",
    "\n",
    "Since I assume there is a lot of overlap in tokens, i.e. a finite vocabulary, we could start also counting how often each token appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 127670),\n",
       " ('to', 69836),\n",
       " ('of', 66705),\n",
       " ('a', 56148),\n",
       " ('and', 52580),\n",
       " ('is', 39554),\n",
       " ('in', 37787),\n",
       " ('I', 37616),\n",
       " ('that', 34582),\n",
       " ('>', 27843)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_frequencies = Counter()\n",
    "for text in bpe_corpus:\n",
    "    token_frequencies.update(text)\n",
    "token_frequencies.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to iterate through the character pairs and take into consideration how often each word appears, keeping in mind the trailing special character.\n",
    "\n",
    "To make the process of splitting and rejoining subword pairs, we will initially split words into their individual characters, using a space as a delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F r o m : </W>',\n",
       " 'l e r x s t @ w a m . u m d . e d u </W>',\n",
       " \"( w h e r e ' s </W>\",\n",
       " 'm y </W>',\n",
       " 't h i n g ) </W>']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token_frequencies = dict()\n",
    "initial_unique_characters = set()\n",
    "\n",
    "for key, value in token_frequencies.items():\n",
    "    characters = list(key)\n",
    "    characters += ['</W>']\n",
    "    initial_unique_characters.update(characters)\n",
    "    new_key = ' '.join(characters)\n",
    "    new_token_frequencies[new_key] = value\n",
    "    \n",
    "list(new_token_frequencies.keys())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the most common character pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = defaultdict(int)\n",
    "\n",
    "for token, value in new_token_frequencies.items():\n",
    "    characters = token.split()\n",
    "    for index in range(len(characters)-1):\n",
    "        pairs[characters[index], characters[index+1]] += value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', '</W>')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_pair = max(pairs, key=pairs.get)\n",
    "most_common_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now having found the most common pair, we need to save this pair in our vocab and modify the keys of the token frequences to reflect this merge. After that we can continue to find the next most common pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(initial_unique_characters)\n",
    "joined_pair = ''.join(most_common_pair)\n",
    "vocab.append(joined_pair)\n",
    "\n",
    "merged_token_frequencies = dict()\n",
    "for token, value in new_token_frequencies.items():\n",
    "    characters = token.split()\n",
    "    new_characters = []\n",
    "\n",
    "    while len(characters) > 1:\n",
    "        merged_characters = ''.join(characters[0:2])\n",
    "        if merged_characters in vocab:\n",
    "            new_characters.append(merged_characters)\n",
    "            characters.pop(1)\n",
    "            characters.pop(0)\n",
    "        else:\n",
    "            new_characters.append(characters[0])\n",
    "            characters.pop(0)\n",
    "    if len(characters) == 1:\n",
    "        new_characters.append(characters[0])\n",
    "    new_token = ' '.join(new_characters)\n",
    "    merged_token_frequencies[new_token] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the whole process more convenient we could refactor everything into a tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bpe_tokenizer(object):\n",
    "    \n",
    "    def __init__(self, corpus, n_merges=10):\n",
    "        self.corpus = corpus\n",
    "        self.n_merges = n_merges\n",
    "        self.n_merges_done = 0\n",
    "        self.token_counts = self.count_tokens()\n",
    "        self.vocab = self.initialize_vocab()\n",
    "\n",
    "    def make_bpe_style_tokens(self, token_counts):\n",
    "        new_token_counts = dict()\n",
    "\n",
    "        for key, value in token_counts.items():\n",
    "            characters = list(key)\n",
    "            characters += ['</W>']\n",
    "            new_key = ' '.join(characters)\n",
    "            new_token_counts[new_key] = value\n",
    "        return new_token_counts\n",
    "\n",
    "    def count_tokens(self):\n",
    "        tokenizer = Tokenizer(nlp.vocab)\n",
    "        token_counts = Counter()\n",
    "        for text in tokenizer.pipe(self.corpus):\n",
    "            tokens = [t.text for t in text if not t.is_punct and not t.is_space]\n",
    "            token_counts.update(tokens)\n",
    "        bpe_token_counts = self.make_bpe_style_tokens(token_counts)\n",
    "        return bpe_token_counts\n",
    "\n",
    "    def initialize_vocab(self):\n",
    "        vocab = set()\n",
    "        for token in self.token_counts.keys():\n",
    "            characters = token.split()\n",
    "            vocab.update(characters)\n",
    "        return list(vocab)\n",
    "    \n",
    "    def find_most_common_pair(self):\n",
    "        pairs = defaultdict(int)\n",
    "\n",
    "        for token, value in self.token_counts.items():\n",
    "            characters = token.split()\n",
    "            for index in range(len(characters)-1):\n",
    "                pairs[characters[index], characters[index+1]] += value\n",
    "        most_common_pair = max(pairs, key=pairs.get)\n",
    "        return most_common_pair\n",
    "    \n",
    "    def modify_vocab(self, new_character):\n",
    "        self.vocab.append(new_character)\n",
    "    \n",
    "    def modify_tokens(self, new_character):\n",
    "        modified_token_frequencies = dict()\n",
    "        for token, value in self.token_counts.items():\n",
    "            characters = token.split()\n",
    "            new_characters = []\n",
    "\n",
    "            while len(characters) > 1:\n",
    "                merged_characters = ''.join(characters[0:2])\n",
    "                if merged_characters in self.vocab:\n",
    "                    new_characters.append(merged_characters)\n",
    "                    characters.pop(1)\n",
    "                    characters.pop(0)\n",
    "                else:\n",
    "                    new_characters.append(characters[0])\n",
    "                    characters.pop(0)\n",
    "            if len(characters) == 1:\n",
    "                new_characters.append(characters[0])\n",
    "            new_token = ' '.join(new_characters)\n",
    "            modified_token_frequencies[new_token] = value\n",
    "        self.token_counts = modified_token_frequencies\n",
    "    \n",
    "    def merge(self, n=None):\n",
    "        if n is None:\n",
    "            n=self.n_merges\n",
    "        for i in range(n):\n",
    "            pair = self.find_most_common_pair()\n",
    "            merged_pair = ''.join(pair)\n",
    "            self.modify_vocab(merged_pair)\n",
    "            self.modify_tokens(merged_pair)\n",
    "        self.n_merges_done += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = bpe_tokenizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e</W>', 'th', 's</W>', 't</W>', 'in', 'er', 'an', 'd</W>', 'on']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.vocab[-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text they use a simpler example, however to compare we can check it with the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = ['low'] * 5\n",
    "example_text += ['lowest'] * 2\n",
    "example_text += ['newer'] * 6\n",
    "example_text += ['wider'] * 3\n",
    "example_text += ['new'] * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokenizer = bpe_tokenizer(example_text)\n",
    "example_tokenizer.merge(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r',\n",
       " 'i',\n",
       " 's',\n",
       " 'd',\n",
       " 'o',\n",
       " 't',\n",
       " '</W>',\n",
       " 'e',\n",
       " 'w',\n",
       " 'l',\n",
       " 'n',\n",
       " 'er',\n",
       " 'er</W>',\n",
       " 'ne',\n",
       " 'new',\n",
       " 'lo',\n",
       " 'low',\n",
       " 'newer</W>',\n",
       " 'low</W>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funny enough, the example provided in the book highlights a point of ambiguity in the description. It isn't clear how to break ties, for example, 'er' or 'r<\\/W>'. Since the algorithm iteratively merges characters, this can have the effect of changing how the next characters are parsed too. \n",
    "\n",
    "Despite this ambiguity we also see that the texts example, and the given code agree on the last 4 merges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond using character pair frequencies as the decisive metric, tokenizers like **Wordpiece** do so by minimizing the language model likelihood of the training data and then using a greedy longest-match to split new documents into word pieces. This approach gets discussed in more detail in future chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes:\n",
    "\n",
    "Below in a reference implementation copied from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'r')\n",
      "('er', '</w>')\n",
      "('n', 'e')\n",
      "('ne', 'w')\n",
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('new', 'er</w>')\n",
      "('low', '</w>')\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = dict()\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "num_merges = 8\n",
    "vocab = {\n",
    "    'l o w </w>':5,\n",
    "    'l o w e s t </w>':2,\n",
    "    'n e w e r </w>':6,\n",
    "    'w i d e r </w>':3,\n",
    "    'n e w </w>':2,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
