{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling and N-grams\n",
    "\n",
    "At the core of many NLP tasks, is the practice of assigning probabilities to a given string. This is referred to as **language modeling** and has in recent years seen a huge amount of development.\n",
    "\n",
    "Assuming one can assign probabilities to a string we can complete tasks such as:\n",
    "\n",
    "- predicting the next word in a sentence (auto-complete)\n",
    "- correctly parse speech to text\n",
    "- correct grammatical errors in written text\n",
    "- assess the quality of translations\n",
    "- measure the probability or likelihood of a given string\n",
    "\n",
    "Modern approaches rely on deep learning and massive language models + data (Bert and co.) which has proven to be formidable solution. In contrast, n-grams is a relatively simple and naive approach based on token frequencies. Despite their age however, these models offer the advantage of:\n",
    "\n",
    "- flexibility in adapting and transforming data for your corpus\n",
    "- relatively light weight\n",
    "\n",
    "Furthermore, for scenarios where you have very unique or scarce data, or if you have high compute/latency costs, then n-grams based approaches might be worth considering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "When trying to model the next word in a sentence, we could take the frequentist approach and make a guess by first referring to a large corpus and counting up how frequently the given sequence occurs, and the number of times its various continuations appear.\n",
    "\n",
    "Thus we could pretty well predict the next word in a sequence like: \n",
    "\n",
    "$$ P(\\textit{blue}\\vert \\textit{the sky is}) =  C(\\textit{the sky is blue} \\div C(\\textit{the sky is})$$\n",
    "\n",
    "Despite the fact this works for some cases, it isn't without issues. First, language is not static like our corpus, it is constantly evolving. Permutations of words which generate sensible sentences is far larger than any corpus. Just because we may never encounter the sentence *\"the sky is red\"* in our reference corpus doesn't mean we should necessarily assign it a probability of 0. In addition, if we want to estimate the probability of *\"the sky is blue\"* would require us to count the frequency of this sentence, and divide in by the count of all 4 word sentences. \n",
    "\n",
    "Although this naive approach isn't ideal, we can make some assumptions to simplify the task. Transforming the join probability using the **chain rule**\n",
    "\n",
    "$$ P(w^n_1)  = P(w_1)P(w_2\\vert w_1)P(w_3 \\vert w_1^2)...P(w_n \\vert w_1^{n-1}) = \\prod_{k=1}^n P(w_k \\vert w_1^{k-1}) $$\n",
    "\n",
    "and then make a **markov assumption** such that:\n",
    "\n",
    "$$ P(w_n \\vert w_1^{n-1}) \\approx P(w_n \\vert w_n-1) $$\n",
    "\n",
    "Thus allowing us simplify our joint probability to:\n",
    "\n",
    "$$ P(w_1^n) \\approx \\prod_{k=1}^n P(w_k \\vert w_k-1) $$\n",
    "\n",
    "Where we can use the **MLE** to estimate the individual probabilities:\n",
    "\n",
    "$$ P(w_k \\vert w_k-1) = \\dfrac{C(w_k w_{k-1})}{\\sum_{w_i} C(w_{k-1} w_i)} $$\n",
    "\n",
    "\n",
    "What is important to note, is that in our **markov assumption** above, we assume the next word only depends on the previous word. This is referred to as a bigram model, in practice we could also predict words based on multiple preceding words to capture more contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bigram Example \n",
    "\n",
    "The data I'll use for this example originates from [here](http://help.sentiment140.com/for-students/)\n",
    "\n",
    "It is tweets which were scraped and they used emojis to determine the sentiment labels. From the website this is the described structure:\n",
    "\n",
    "`\n",
    "    The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "    0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    1 - the id of the tweet (2087)\n",
    "    2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    4 - the user that tweeted (robotickilldozr)\n",
    "    5 - the text of the tweet (Lyx is cool)\n",
    "`\n",
    "\n",
    "This will provide an interesting dataset with many issues seen in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path(os.getcwd())\n",
    "TRAIN_CSV = CWD / 'training.1600000.processed.noemoticon.csv'\n",
    "TEST_CSV = CWD / 'testdata.manual.2009.06.14.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['polarity', 'id', 'date','query','user','text']\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV, encoding='latin-1', header=None)\n",
    "test_df = pd.read_csv(TEST_CSV, encoding = 'latin-1', header=None)\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical issue which immediately arises when trying to load this data is that it is not `utf-8` encoded.\n",
    "\n",
    "You can try a bash command like\n",
    "```\n",
    "file -i *\n",
    "``` \n",
    "It will give you a best guess, otherwise simply trying `ascii` and `latin-1` are reasonable guesses for english especially. If you want to read more about this problem which all too often arises in the wild read [here](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/) or take a look on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity          id                          date     query  \\\n",
       "0               0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1               0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2               0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3               0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4               0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "1599995         4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996         4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997         4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998         4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999         4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in chapter 2, some of the question we're now confronted with are the following:\n",
    "\n",
    "- how do we tokenize the data\n",
    "- is punctuation a token\n",
    "    - should repeated punctuation be normalized (!!! -> !)\n",
    "- should we lowercase everything\n",
    "- should we replace/standardize u\n",
    "    - usernames\n",
    "    - hashtags\n",
    "    - numbers\n",
    "    - urls\n",
    "\n",
    "Depending on the downstream some of these things might need to be done. For thoroughness we'll just implement all of these steps, that way the data has fewer unique tokens.\n",
    "\n",
    "First let's have a look at usernames and hashtags since I'm not 100% familiar with how they can be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "tags = []\n",
    "for text in train_df.text.values:\n",
    "    rough_tokens = text.split(' ')\n",
    "    for token in rough_tokens:\n",
    "        if '@' in token:\n",
    "            users.append(token)\n",
    "        if '#' in token:\n",
    "            tags.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@switchfoot',\n",
       "  '@Kenichan',\n",
       "  '@nationwideclass',\n",
       "  '@Kwesidei',\n",
       "  '@LOLTrish',\n",
       "  '@Tatiana_K',\n",
       "  '@twittera',\n",
       "  '@caregiving',\n",
       "  '@octolinz16',\n",
       "  '@smarrison'],\n",
       " ['#itm',\n",
       "  '#therapyfail',\n",
       "  '#fb',\n",
       "  '#TTSC?',\n",
       "  '#24',\n",
       "  '#gayforpeavy',\n",
       "  '#FML',\n",
       "  '#3',\n",
       "  '#camerafail',\n",
       "  '#'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[0:10],tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a few regex to help us identify these texts as well as pipe everything through `spaCy` and rely on some of the tools they provide.\n",
    "\n",
    "In a strictly python based approach we could use the `re` library. However, in this case we'll use some built in functionality from `spaCy` this provides neater code and makes it reusable down the road.\n",
    "\n",
    "Thus we'll refer to the [docs](https://spacy.io/usage/rule-based-matching#regex), and build a couple regex patterns. Note, these regex patters need to work on the already tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner','parser','tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hashtag_matcher(object):\n",
    "    \n",
    "    def __init__(self, nlp, label='HASHTAG'):\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension(\"is_hashtag\", default=False, force=True)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_tech_org == True.\n",
    "        Doc.set_extension(\"has_hashtag\", getter=self.has_tech_org, force=True)\n",
    "        Span.set_extension(\"has_hashtag\", getter=self.has_tech_org, force=True)\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        hashtags = []\n",
    "        for match_id, start, end in matches:\n",
    "            if doc.vocab.strings[match_id] == \"HASHTAG\":\n",
    "                hashtags.append(doc[start:end])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in hashtags:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token._.is_hashtag = True\n",
    "        return doc  # don't forget to return the Doc!\n",
    "            \n",
    "        \n",
    "    def has_tech_org(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a hashtag. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_hashtag' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get(\"is_hashtag\") for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hashtag_matcher', <__main__.hashtag_matcher at 0x7f1e5b9cf370>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe('hashtag_matcher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_matcher = hashtag_matcher(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(tag_matcher, last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here False\n",
      "is False\n",
      "a False\n",
      "#tweet True\n"
     ]
    }
   ],
   "source": [
    "test_doc = 'here is a #tweet'\n",
    "for token in nlp(test_doc):\n",
    "    print(token, token._.is_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hashtag_matcher']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'hashtag_matcher' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-19110d930007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'hashtag_matcher' object is not iterable"
     ]
    }
   ],
   "source": [
    "list(nlp(test_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_pattern = '^@\\w+$'\n",
    "tag_pattern = '^#\\w+$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pattern = re.compile('\\s(@\\w+)\\s')\n",
    "tag_pattern = re.compile('[\\s^](#\\w+)\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#asbc <USER> hi there #as'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_pattern = re.compile('[\\s^](#\\w+)[\\s$]')\n",
    "test = '#asbc #asdf hi there #as'\n",
    "tag_pattern.sub(' <USER> ', test, count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match=' #asbc '>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_pattern.match(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #asbc hi there'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ' #asbc hi there'\n",
    "user_pattern.sub('<USER>', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_pattern.sub??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\",\n",
       "       \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n",
       "       '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds',\n",
       "       ..., 'Are you ready for your MoJo Makeover? Ask me for details ',\n",
       "       'Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur ',\n",
       "       'happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.text.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Comments:\n",
    "\n",
    "If you don't want to go through the trouble of counting n-grams yourselves, there are also a few resources available from very large corpus available here:\n",
    "\n",
    "[google n-grams](https://storage.googleapis.com/books/ngrams/books/datasetsv2.html)\n",
    "[5-grams](https://catalog.ldc.upenn.edu/LDC2006T13)\n",
    "[historical british 1,2,3-grams](https://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz)\n",
    "[yahoo n-grams](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&guccounter=1)\n",
    "\n",
    "Additional general NLP data sources can also be found [here](https://github.com/niderhoff/nlp-datasets) with some broken links. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
